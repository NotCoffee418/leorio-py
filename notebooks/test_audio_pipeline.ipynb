{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import importlib\n",
    "from IPython.display import Audio\n",
    "import pyaudio\n",
    "import wave\n",
    "import time\n",
    "from scipy.signal import butter, filtfilt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logic.utils.io_access as io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# force reload imports\n",
    "#importlib.reload(at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_seed_device_index():\n",
    "    p = pyaudio.PyAudio()\n",
    "    device_count = p.get_device_count()\n",
    "    target_description = \"seeed-2mic-voicecard\"\n",
    "\n",
    "    for i in range(device_count):\n",
    "        device_info = p.get_device_info_by_index(i)\n",
    "        if device_info.get('maxInputChannels') > 0:\n",
    "            device_name = device_info.get('name')\n",
    "            if target_description in device_name:\n",
    "                p.terminate()\n",
    "                return i\n",
    "\n",
    "    p.terminate()\n",
    "    raise Exception(\n",
    "        f\"Device with description containing '{target_description}' not found\")\n",
    "device_index = find_seed_device_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytes_to_float32(bytes_data):\n",
    "    return np.frombuffer(bytes_data, dtype=np.float32)\n",
    "\n",
    "def bytes_to_int16(bytes_data):\n",
    "    return np.frombuffer(bytes_data, dtype=np.int16)\n",
    "\n",
    "def float32_to_bytes(np_data):\n",
    "    return np_data.astype(np.float32).tobytes()\n",
    "\n",
    "def int16_to_bytes(np_data):\n",
    "    return np_data.astype(np.int16).tobytes()\n",
    "\n",
    "def float32_to_int16(float32_array):\n",
    "    # Clip, prevent overload\n",
    "    float32_array = np.clip(float32_array, -1.0, 1.0)\n",
    "    \n",
    "    # Scale it to int16 values\n",
    "    float32_array = np.round(float32_array * 32767)\n",
    "\n",
    "    # Cast and return it\n",
    "    return float32_array.astype(np.int16)\n",
    "\n",
    "def filter_human_speech_only(np_channel, sample_rate):\n",
    "    # Ensure the data type\n",
    "    np_channel = np_channel.astype(np.float32)\n",
    "\n",
    "    # Normalize the audio data if it is not already between -1 and 1\n",
    "    if np.max(np.abs(np_channel)) > 1:\n",
    "        np_channel = np_channel / np.max(np.abs(np_channel))\n",
    "\n",
    "    # Filter design parameters\n",
    "    order = 5  # <-- You might want to adjust this\n",
    "    nyquist = 0.5 * sample_rate\n",
    "    low = 300.0 / nyquist\n",
    "    high = 3400.0 / nyquist\n",
    "\n",
    "    # Design and apply the filter\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    y = filtfilt(b, a, np_channel)\n",
    "\n",
    "    # Make sure the output is in float32 format\n",
    "    y = y.astype(np.float32)\n",
    "\n",
    "    return y\n",
    "\n",
    "def volume_boost(np_channel, volume_ratio):\n",
    "    # Volume boost & prevent clipping\n",
    "    return np.clip(np_channel * volume_ratio, -1, 1)\n",
    "\n",
    "\n",
    "def seperate_channels(np_data):\n",
    "    left_channel = np_data[::2]\n",
    "    right_channel = np_data[1::2]\n",
    "    return left_channel, right_channel\n",
    "\n",
    "\n",
    "def merge_two_channels(np_left_channel, np_right_channel):\n",
    "    return (np_left_channel / 2) + (np_right_channel / 2)\n",
    "\n",
    "def clip_float32(np_data):\n",
    "    return np.clip(np_data, -1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channels_count = 2\n",
    "output_channels_count = 1\n",
    "sample_rate = 44100\n",
    "record_seconds = 5\n",
    "testfile_path = io.get_path('data', 'test.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechAudioStreamObservable:\n",
    "    def __init__(self):\n",
    "        # Set hardcoded values\n",
    "        self.rate = sample_rate\n",
    "        self.channels = input_channels_count\n",
    "        self.format = pyaudio.paFloat32\n",
    "        self.frames_per_buffer = 1024\n",
    "        self.input_device_index = device_index\n",
    "        self.observers = []\n",
    "\n",
    "        self.p = pyaudio.PyAudio()\n",
    "        self.stream = self.p.open(\n",
    "            format=self.format,\n",
    "            channels=self.channels,\n",
    "            rate=self.rate,\n",
    "            input_device_index=self.input_device_index,\n",
    "            input=True,\n",
    "            frames_per_buffer=self.frames_per_buffer,\n",
    "            stream_callback=self._callback\n",
    "        )\n",
    "\n",
    "    def _callback(self, in_data, frame_count, time_info, status):\n",
    "        # Convert to np & seperate stereo channels\n",
    "        np_f32 = bytes_to_float32(in_data)\n",
    "        left, right = seperate_channels(np_f32)\n",
    "\n",
    "        # Filter to human speech frequencies\n",
    "        # WIP: We need fix whitenoise first, then test again\n",
    "        #left = filter_human_speech_only(left, self.rate)\n",
    "        #right = filter_human_speech_only(right, self.rate)\n",
    "\n",
    "        # Volume boost\n",
    "        # left = volume_boost(left, volume_rate)\n",
    "        # right = volume_boost(right, volume_rate)\n",
    "\n",
    "        # Merge channels to single mono channel\n",
    "        np_f32 = merge_two_channels(left, right)\n",
    "        \n",
    "        # Clip to prevent overload\n",
    "        np_f32 = clip_float32(np_f32)\n",
    "        \n",
    "        # Convert to int16 binary for output\n",
    "        np_i16 = float32_to_int16(np_f32)\n",
    "        out_binary_i16 = int16_to_bytes(np_i16)\n",
    "        \n",
    "        \n",
    "        # Report data to observer\n",
    "        for observer in self.observers:\n",
    "            observer.on_received(out_binary_i16)\n",
    "        return (np_f32, pyaudio.paContinue)\n",
    "\n",
    "    def start(self):\n",
    "        try:\n",
    "            self.stream.start_stream()\n",
    "        except Exception as ex:\n",
    "            print(f\"An error occurred while starting the stream: {ex}\")\n",
    "\n",
    "    def stop(self):\n",
    "        try:\n",
    "            self.stream.stop_stream()\n",
    "            self.stream.close()\n",
    "            self.p.terminate()\n",
    "        except Exception as ex:\n",
    "            print(f\"An error occurred while stopping the stream: {ex}\")\n",
    "\n",
    "    def add_observer(self, observer):\n",
    "        self.observers.append(observer)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class AudioDataObserver:\n",
    "    def __init__(self, duration):\n",
    "        self.filename = testfile_path\n",
    "        self.rate = sample_rate\n",
    "        self.channels = output_channels_count\n",
    "        self.duration = duration\n",
    "        self.binary_audio_data = bytearray()\n",
    "        self.frames = int(sample_rate * duration)\n",
    "        self.frame_count = 0\n",
    "\n",
    "    def on_received(self, audio_data):\n",
    "        self.binary_audio_data.extend(audio_data)\n",
    "    \n",
    "    # this is binary int16 data\n",
    "    def get_binary_audio_data(self):\n",
    "        return self.binary_audio_data\n",
    "    \n",
    "    def clear_audio_data():\n",
    "        self.binary_audio_data = bytearray()\n",
    "        self.frame_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record 5 seconds of audio\n",
    "audio_stream_observable = SpeechAudioStreamObservable()\n",
    "audio_stream_observer = AudioDataObserver(record_seconds)\n",
    "audio_stream_observable.add_observer(audio_stream_observer)\n",
    "\n",
    "# Start the audio stream\n",
    "audio_stream_observable.start()\n",
    "try:\n",
    "    print(\"Recording audio for 3 seconds...\")\n",
    "    time.sleep(record_seconds)\n",
    "finally:\n",
    "    print(\"Done recording.\")\n",
    "    audio_stream_observable.stop()\n",
    "    \n",
    "# Get recording binary\n",
    "binary_recording_data = audio_stream_observer.get_binary_audio_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy array waves\n",
    "np_input = bytes_to_int16(binary_recording_data)\n",
    "\n",
    "# In case of two output channels\n",
    "np_left, np_right = seperate_channels(np_input)\n",
    "display(Audio(np_left, rate=sample_rate))\n",
    "display(Audio(np_right, rate=sample_rate))\n",
    "\n",
    "# In case of one output channel\n",
    "display(Audio(np_input, rate=sample_rate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize recoding\n",
    "# Create the plots\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 6))\n",
    "\n",
    "# Plot left channel\n",
    "axs[0].plot(np_left)\n",
    "axs[0].set_title('Left Channel')\n",
    "axs[0].set_xlabel('Sample number')\n",
    "axs[0].set_ylabel('Amplitude')\n",
    "\n",
    "# Plot right channel\n",
    "axs[1].plot(np_right)\n",
    "axs[1].set_title('Right Channel')\n",
    "axs[1].set_xlabel('Sample number')\n",
    "axs[1].set_ylabel('Amplitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Min after processing:\", np.min(np_input))\n",
    "print(\"Max after processing:\", np.max(np_input))\n",
    "print(\"Input data type:\", np_input.dtype)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
